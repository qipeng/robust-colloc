---
title: "Robust Collocation Extraction"
author: "Peng Qi & Ruoqing Jiang"
output: html_document
---

Set up some basic variables. `map` is the mapping from indices to words, `corpus` is the corpus we will be working with, `V` is the vocabulary size (i.e. number of distinct words). `IJ[k,]` stores the indices of the first and second words in the bigram indexed `k`, and `idxlookup` is a lookup table that yields this index given the two words' indices.
```{r}
library(Matrix)
library(plyr)

if (file.exists('setup.RData')) {
  load('setup.RData')
} else {
  map = read.table("nyt/nyt_eng_200812.map", quote="")
  
  inds = read.table("nyt/nyt_eng_200812.ind")
  corpus = simplify2array(inds)
  
  n<-length(corpus)-1
  V<-length(unique(corpus))
  
  IJ = matrix(0, n, 2)
  
  for (i in 1:n) {
    IJ[i,] = corpus[i:(i+1)]
  }
  IJ = data.frame(IJ)
  IJ = ddply(IJ, .(IJ$X1, IJ$X2), nrow)
  IJ = simplify2array(IJ)[,2:1]
  
  npairs = dim(IJ)[1]
  
  idxlookup = sparseMatrix(i = IJ[,1], j = IJ[,2], x = 1:npairs, dims = c(V, V))
  
  save(map, corpus, V, IJ, npairs, idxlookup, file = 'setup.RData')
}

print(sprintf("# Words: %d", length(corpus)))
print(sprintf("# Unique Words: %d", V))
print(sprintf("# Unique Bigrams: %d", npairs))
```
More setup: count frequencies of words (`frequency`) and bigrams (`nfrequency`).
```{r}
count_freqs = function(corpus, idxlookup, IJ, V) {
  # This function has been optimized for performance.
  # Using for loops too much will be impossibly slow.
  npairs = dim(IJ)[1]
  n = length(corpus)
  
  IJ1 = matrix(0, n-1, 2)
  for (i in 1:(n-1)) {
    IJ1[i,] = corpus[i:(i+1)]
  }
  IJ1 = data.frame(IJ1)
  IJ1 = ddply(IJ1, .(IJ1$X1, IJ1$X2), nrow)
  IJ1 = simplify2array(IJ1)
  
  nfrequency = rep(0, npairs)
  nfrequency[idxlookup[IJ1[,2:1]]] = IJ1[,3]
  
  frequency = rep(0, V)
  counts = table(corpus[1:(n-1)])
  frequency[as.numeric(rownames(counts))] = counts
  
  res = list()
  res$nfrequency = nfrequency
  res$frequency = frequency
  res
}

if (file.exists('setup2.RData')) {
  load('setup2.RData')
} else {
  res = count_freqs(corpus, idxlookup, IJ, V)
  nfrequency = res$nfrequency
  frequency = res$frequency
  
  save(nfrequency, frequency, file = 'setup2.RData')
}
```
Last bit of setup: build language model by MLE.
```{r}
if (file.exists('setup3.RData')) {
  load('setup3.RData')
} else {
  conditional = rep(0, npairs)
  
  conditional[idxlookup[IJ]] = nfrequency[idxlookup[IJ]] / frequency[IJ[,2]]
  save(conditional, file = 'setup3.RData')
}
```
For efficient sampling from the languge model (conditional distributions), we implemented samplers with the [alias method](https://en.wikipedia.org/wiki/Alias_method).
```{r}
library(hash)

if (file.exists('samplers.RData')) {
  load('samplers.RData')
} else {
  # initialize multinomial samplers using alias method
  x = hash()
  J = hash()
  q = hash()
  
  for (i in 1:V) {
    x[i] = which(idxlookup[,i] > 0, arr.ind = T)
    probs = conditional[idxlookup[,i]]
    K = length(probs)
    qq = rep(0, K)
    JJ = rep(0, K)
    
    smaller = c()
    larger = c()
    for (j in 1:K) {
      qq[j] = K * probs[j]
      JJ[j] = j
      if (qq[j] < 1) {
        smaller = append(smaller, j)
      } else {
        larger = append(larger, j)
      }
    }
    
    while (length(smaller) > 0 && length(larger) > 0) {
      small = smaller[1]; smaller = smaller[-1];
      large = larger[1]; larger = larger[-1];
      
      JJ[small] = large
      qq[large] = qq[large] + qq[small] - 1.0
      
      if (qq[large] < 1.0) {
        smaller = append(smaller, large)
      } else {
        larger = append(larger, large)
      }
    }
    
    q[i] = qq
    J[i] = JJ
  }
  save(x, J, q, file = 'samplers.RData')
}

draw.alias = function(keyi) {
  xx = x[[keyi]] 
  JJ = J[[keyi]]
  qq = q[[keyi]]
  K = length(xx)
  k = ceiling(runif(1) * K)
  if (k == 0) {
    k = 1
  } else if (k > length(qq)) {
    k = length(qq)
  }
  if (runif(1) >= qq[k]) {
    return(xx[JJ[k]])
  } else {
    return(xx[k])
  }
}
```
Important functions: 
  
* `draw.sample` defines how we are drawing Markov chain bootstrap samples given the transition probabilities `conditional` and sample length `size`;
* `PMIfamily` computes multiple PMI-related statistics given a corpus, sharing the collection of sufficient statistics among these to save computational time.
* `PMI`, `PMIsq`, and `NMPI` implements point-wise mutual information, a modified PMI$^2$, and NPMI, respectively.
```{r}
draw.sample<-function(conditional,size,RNG=.Random.seed) {
  # for parallel
  set.seed(RNG)
  
  new<-rep(0,size)
  new[1]<-1
  mp = hash()
  mx = hash()
  
  for (i in 2:size) {
    lastkey = as.character(new[i-1])
    new[i] = draw.alias(lastkey)
  }
  
  return(new)
}

PMI = function(probw1w2, probw1, probw2) {
  if (probw1w2 == 0) {
    probw1w2 = 1e-100
  }
  if ((probw1!=0)&&(probw2!=0)) {
    res = log(probw1w2/(probw1*probw2))
  } else {
    res = NaN
  }
  res
}

PMIsq = function(probw1w2, probw1, probw2) {
  if (probw1w2 == 0) {
    probw1w2 = 1e-100
  }
  if ((probw1!=0)&&(probw2!=0)) {
    res = log(probw1w2^2/(probw1*probw2)) / log(probw1w2)
  } else {
    res = NaN
  }
  res
}

NPMI = function(probw1w2, probw1, probw2) {
  if (probw1w2 == 0) {
    probw1w2 = 1e-100
  }
  if ((probw1!=0)&&(probw2!=0)) {
    res = log(probw1w2/(probw1*probw2)) / -log(probw1w2)
  } else {
    res = NaN
  }
  res
}

PMIfamily<-function(functions, corpus, V, idxlookup, IJ) {
  npairs = dim(IJ)[1]
  n = length(corpus)
  res = count_freqs(corpus, idxlookup, IJ, V)
  nfrequency = res$nfrequency
  frequency = res$frequency
  
  nprob = nfrequency / (n - 1)
  prob = frequency / n
  
  res = matrix(0, npairs, length(functions))
  for(i in 1:npairs) {
    w1 = IJ[i,1]
    w2 = IJ[i,2]
    for (j in 1:length(functions)) {
      res[i, j] = functions[[j]](nprob[i], prob[w1], prob[w2])
    }
  }
  res
}
```
Compute statistics on the original corpus.
```{r}
stat_names = c('PMI', 'PMI^2*', 'NPMI')
perplexity = function(conditional, corpus, idxlookup) {
  n = length(corpus)
  exp(mean(log(1 / conditional[idxlookup[cbind(corpus[2:n], corpus[1:(n-1)])]])))
}
if (file.exists('vanilla.RData')) {
  load('vanilla.RData')
} else {
  vanilla_res = PMIfamily(c(PMI, PMIsq, NPMI), corpus, V, idxlookup, IJ)
  ppl = perplexity(conditional, corpus, idxlookup)
  save(vanilla_res, ppl, file = 'vanilla.RData')
}
vanilla_res[1:10,]
print(sprintf('The perplexity of the MLE language model is: %f', ppl))
```
Finally, run bootstrap!
```{r}
library(parallel)
Nshards = 25
SAMPLES = 8
CORES = 8
N = Nshards * SAMPLES

# set up RNGs
RNGkind("L'Ecuyer-CMRG")

RNGS = list()
s = .Random.seed
for (i in 1:N) {
  s = nextRNGSubStream(s)
  RNGS[[i]] = s
}

# 500, RNGS[[i + (shard - 1) * SAMPLES]]

for (shard in 1:Nshards) {
  filename = sprintf('bootstrap.part%d.RData', shard)
  if (!file.exists(filename)) {
    bootstrap_res = mclapply(1:SAMPLES, function(i) PMIfamily(c(PMI, PMIsq, NPMI), draw.sample(conditional, length(corpus), RNGS[[i + (shard - 1) * SAMPLES]]),V,idxlookup,IJ), mc.cores = CORES)
    save(bootstrap_res, file = filename)
  }
}
```
Collect statistics from bootstrap samples.
```{r}
if (file.exists('bootstrap.RData')) {
  load('bootstrap.RData')
} else {
  bootstrap_results = list()
  for (shard in 1:Nshards) {
    filename = sprintf('bootstrap.part%d.RData', shard)
    load(filename)
    bootstrap_results[[length(bootstrap_results) + 1]] = bootstrap_res
  }
  bootstrap_results = simplify2array(bootstrap_results)
  
  N = Nshards * SAMPLES
  I = dim(bootstrap_results[[1]])[1]
  J = dim(bootstrap_results[[1]])[2]
  
  bootstrap_res_mat = matrix(0, N, I * J)
  for (i in 1:N) {
    bootstrap_res_mat[i,] = bootstrap_results[[i]]
  }
  
  npairs = I
  nstats = J
  
  # replace NaNs with null
  for (istat in 1:nstats) {
    stat_name = stat_names[istat]
    if (stat_name == 'PMI') {
      for (col in (istat-1) * npairs + (1:npairs)) {
        bootstrap_res_mat[is.nan(bootstrap_res_mat[,col]),col] = 0
      }
    } else if (stat_name == 'PMI^2*') {
      for (col in (istat-1) * npairs + (1:npairs)) {
        bootstrap_res_mat[is.nan(bootstrap_res_mat[,col]),col] = 1
      }
    } else if (stat_name == 'NPMI') {
      for (col in (istat-1) * npairs + (1:npairs)) {
        bootstrap_res_mat[is.nan(bootstrap_res_mat[,col]),col] = 0
      }
    }
  }
  
  save(N, I, J, bootstrap_res_mat, npairs, nstats, file = 'bootstrap.RData')
}

alpha = .95   # Confidence level

if (file.exists('bootstrap.stats.RData')) {
  load('bootstrap.stats.RData')
} else {
  statistics = apply(bootstrap_res_mat, 2, function(x) c(mean(x), quantile(x, probs = c((1-alpha)/2, (1+alpha)/2))))
  meanstat = statistics[1,]
  CIlow = statistics[2,]
  CIhigh = statistics[3,]
  
  meanstat = matrix(meanstat, I, J)
  CIlow = matrix(CIlow, I, J)
  CIhigh = matrix(CIhigh, I, J)
  
  rejectedLow = CIlow
  rejectedLow[,1] = (rejectedLow[,1] <= 0)
  rejectedLow[,2] = (rejectedLow[,2] <= 1)
  rejectedLow[,3] = (rejectedLow[,3] <= 0)
  rejectedHigh = CIhigh
  rejectedHigh[,1] = (rejectedHigh[,1] >= 0)
  rejectedHigh[,2] = (rejectedHigh[,2] >= 1)
  rejectedHigh[,3] = (rejectedHigh[,3] >= 0)
  rejected = rejectedLow & rejectedHigh
  
  save(meanstat, CIlow, CIhigh, rejected, file = 'bootstrap.stats.RData')
}
```
Present statstics (for LaTeX)
```{r, echo = FALSE}
topN = 10
rndN = 30
randidx = sample.int(I, rndN)

printTopN = function(res) {
  topIdx = matrix(0, topN, J)
  for (j in 1:J) {
    ranked = simplify2array(sort.int(res[,j], decreasing = (j!=2), index.return = T)$ix)
    topIdx[1:topN,j] = ranked[1:topN]
  }
  for (i in 1:topN) {
    score1 = sprintf('%.2f', res[topIdx[i,1],1])
    score2 = sprintf('%.2f', res[topIdx[i,2],2])
    score3 = sprintf('%.2f', res[topIdx[i,3],3])
    
    print(sprintf("%d & %s %s & %s & %s %s & %s & %s %s & %s \\",
                  i, map[IJ[topIdx[i,1],2],2], map[IJ[topIdx[i,1],1],2], score1, 
                  map[IJ[topIdx[i,2],2],2], map[IJ[topIdx[i,2],1],2], score2,
                  map[IJ[topIdx[i,3],2],2], map[IJ[topIdx[i,3],1],2], score3))
  }
}

printRand = function(res, res2 = NULL, rejected = NULL) {
  for (i in 1:rndN) {
    if (!is.null(res2)) {
      score1 = sprintf('[%.2f,%.2f]', res[randidx[i],1], res2[randidx[i],1])
      score2 = sprintf('[%.2f,%.2f]', res[randidx[i],2], res2[randidx[i],2])
      score3 = sprintf('[%.2f,%.2f]', res[randidx[i],3], res2[randidx[i],3])
      if (!is.null(rejected)) {
        if (!rejected[randidx[i],1]) {
          score1 = sprintf('\\textbf{%s}', score1)
        }
        if (!rejected[randidx[i],2]) {
          score2 = sprintf('\\textbf{%s}', score2)
        }
        if (!rejected[randidx[i],3]) {
          score3 = sprintf('\\textbf{%s}', score3)
        }
      }
    } else {
      score1 = sprintf('%.2f', res[randidx[i],1])
      score2 = sprintf('%.2f', res[randidx[i],2])
      score3 = sprintf('%.2f', res[randidx[i],3])
    }
    
    print(sprintf("%s %s & %s & %s & %s \\",
                  map[IJ[randidx[i],2],2], map[IJ[randidx[i],1],2], 
                  score1, score2, score3))
  }
}

print("Table 1")
printTopN(vanilla_res)
print("Table 2")
printRand(vanilla_res)
print("Table 3")
meanstat1 = meanstat
meanstat1[rejected] = 0
printTopN(meanstat1)
print("Table 4")
printRand(CIlow, CIhigh, rejected)
```